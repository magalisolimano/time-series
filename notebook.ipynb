{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 Project\n",
    "\n",
    "**Author: Magali Solimano**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What are the top 5 best zip codes for us to invest in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria: Zipcodes with below-median sales price and annual ROI \n",
    "(over previous 5 years) in 90th percentile.  \n",
    "Scope: The analysis focuses on the state of FL, which exploratory data analysis shows \n",
    "has a high percentage  \n",
    "of zipcodes that meet this criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variable Descriptions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __RegionID__: Unique ID for each region\n",
    "\n",
    "- __RegionName__: Zipcode\n",
    "\n",
    "- __City__: City name\n",
    "\n",
    "- __State__: State name\n",
    "\n",
    "- __Metro__: Name of metro city around region\n",
    "\n",
    "- __CountyName__: County name\n",
    "\n",
    "- __SizeRank__: Rank by size of region\n",
    "\n",
    "- __SalePrice__: Median sales price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import pmdarima as pm\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('data/zillow_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine shape of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine number of unique values for all columns except time series columns\n",
    "df.iloc[:, 0:7].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has over 14,700 entries spanning a twenty-two year period from April 1996 to April 2018. The dataset  \n",
    "includes 50 states and Washington, DC, and it covers 7,554 cities and 701 metro areas. \n",
    "\n",
    "Each row represents a zipcode, which is captured by RegionName in this dataset. Number of rows is  \n",
    "equal to unique entries for RegionName."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns: RegionName refers to zipcode, and CountyName can be renamed to County\n",
    "df.rename({'RegionName': 'Zipcode'}, axis='columns', inplace=True)\n",
    "df.rename({'CountyName': 'County'}, axis='columns', inplace=True)\n",
    "# Set dtype to string\n",
    "df.Zipcode = df.Zipcode.astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values by column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metro has missing values. The time series data is also likely to have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Metro na's with 'None'\n",
    "df.Metro.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for total missing values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group zipcode by city, county, and state\n",
    "df[['Zipcode','City','County','State']].sort_values(by=['Zipcode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of MA's zipcodes in the dataset only have four digits. Looking up the state's  \n",
    "zipcodes, many start with '0', which appear to have been dropped in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert '0' as first digit in zipcodes with only four digits.\n",
    "for i in range(len(df)):\n",
    "    df.Zipcode[i] = df.Zipcode[i].rjust(5, '0')\n",
    "\n",
    "# Return zipcode min to confirm that value now starts with '0'    \n",
    "df.Zipcode.min()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot value counts by state, county, metro, and city\n",
    "# Create figure and set size\n",
    "fig, axes = plt.subplots(nrows=1 , ncols=4, figsize=(15,8))\n",
    "\n",
    "# Plot value counts by state, metro, and city\n",
    "df.State.value_counts().head(50).plot(kind='barh', title='State', ax = axes[0])\n",
    "df.County.value_counts().head(50).plot(kind='barh', title='County', ax = axes[1])\n",
    "df.Metro.value_counts().head(50).plot(kind='barh', title='Metro Area', ax = axes[2])\n",
    "df.City.value_counts().head(50).plot(kind='barh', title='City', ax = axes[3])\n",
    "\n",
    "# Invert y axes\n",
    "axes[0].invert_yaxis()\n",
    "axes[1].invert_yaxis()\n",
    "axes[2].invert_yaxis()\n",
    "axes[3].invert_yaxis()\n",
    "\n",
    "# Set title\n",
    "plt.suptitle('Value Counts by State, County, Metro Area, and City', \n",
    "             y=1.01, fontsize=12)\n",
    "\n",
    "# Format layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for missing data in time series columns\n",
    "for col in reversed(df.columns):\n",
    "    if df[col].isna().sum() > 0:\n",
    "        print(col)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data appears to be missing for some zipcodes in period leading up to June 2014.  \n",
    "How many zipcodes are affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return zipcodes that have missing data in June 2014\n",
    "print(len(df[df['2014-06'].isna()]))\n",
    "df[df['2014-06'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 56 zipcodes have missing data prior to June 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return mean and median prices in 2018 and 2013, and calculate percent change\n",
    "# Mean and median price across all zipcodes in April 2018 and July 2014\n",
    "print('Mean price, April 2018: $', np.round(df.groupby('Zipcode')['2018-04'].mean().mean()))\n",
    "print('Median price, April 2018: $', df.groupby('Zipcode')['2018-04'].median().median())\n",
    "print('-------------------------------------')\n",
    "# Mean and median price across all zipcodes in April 2013\n",
    "print('Mean price, April 2013: $', np.round(df.groupby('Zipcode')['2013-04'].mean().mean()))\n",
    "print('Median price, April 2013: $', df.groupby('Zipcode')['2013-04'].median().median())\n",
    "print('--------------------------------------')\n",
    "# Price percentage change, 2018 vs. 2013\n",
    "print('Mean price, percent change: ', np.round(100*(288040-198100)/198100),'%')\n",
    "print('Median price, percent change: ', np.round(100*(211731-151200)/151200),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean and median sales prices have increased over 5y period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROI metrics\n",
    "# Cumulative 5-year ROI from 2013 to 2018\n",
    "df['roi_cum'] = 100*(df['2018-04'] - df['2013-04']) / df['2013-04']\n",
    "\n",
    "# Annual ROI for most recent 5 full years of data\n",
    "df['roi_13-14'] = 100*(df['2014-04'] - df['2013-04']) / df['2013-04']\n",
    "df['roi_14-15'] = 100*(df['2015-04'] - df['2014-04']) / df['2014-04']\n",
    "df['roi_15-16'] = 100*(df['2016-04'] - df['2015-04']) / df['2015-04']\n",
    "df['roi_16-17'] = 100*(df['2017-04'] - df['2016-04']) / df['2016-04']\n",
    "df['roi_17-18'] = 100*(df['2018-04'] - df['2017-04']) / df['2017-04']\n",
    "\n",
    "# Average annual ROI over 5-year period\n",
    "df['roi_annual_avg_5y'] = (df['roi_13-14'] + df['roi_14-15'] + \n",
    "                           df['roi_15-16'] + df['roi_16-17'] + \n",
    "                           df['roi_17-18']) / 5\n",
    "\n",
    "# Average annual ROI over 3-year period\n",
    "df['roi_annual_avg_3y'] = (df['roi_15-16'] + \n",
    "                        df['roi_16-17'] + df['roi_17-18']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess ROI metrics by zipcode\n",
    "# Return median cumulative ROI for all zipcodes and by each zipcode\n",
    "print('Median ROI from 2013-2018 (%): ', np.round(df.roi_cum.median(),2))\n",
    "\n",
    "# Sort by highest average annual ROI\n",
    "df.sort_values('roi_annual_avg_5y',ascending=False).head(10)[\n",
    "                                    ['Zipcode','City','State',\n",
    "                                     '2018-04','roi_cum',\n",
    "                                     'roi_annual_avg_5y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zipcodes in CO, CA (2), MI, FL (3), TN, PA, and NY have the highest median  \n",
    "annual ROI over the 5y period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for average annual ROI\n",
    "print(df.roi_annual_avg_5y.describe())\n",
    "print('90%: ', df['roi_annual_avg_5y'].quantile(0.90))\n",
    "print('95%: ', df['roi_annual_avg_5y'].quantile(0.95))\n",
    "print('99%: ', df['roi_annual_avg_5y'].quantile(0.99))\n",
    "print('99.9%: ', df['roi_annual_avg_5y'].quantile(0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for sales price in April 2018\n",
    "print(df['2018-04'].describe())\n",
    "print('20%: ', df['2018-04'].quantile(0.20))\n",
    "print('90%: ', df['2018-04'].quantile(0.90))\n",
    "print('95%: ', df['2018-04'].quantile(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sales price and average annual ROI for all states\n",
    "# Create figure and set size\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8,6))\n",
    "\n",
    "# Plot average 2018 sales price by state\n",
    "df.groupby('State')['2018-04'].median().sort_values(\n",
    "                                                ascending=False).plot(\n",
    "                                                kind='bar',\n",
    "                                                color='lightblue',\n",
    "                                                ax=ax[0])\n",
    "# Plot average sales price for all states\n",
    "ax[0].axhline(y=np.nanmedian(df['2018-04']), color='black', \n",
    "              linestyle='--')\n",
    "\n",
    "# Set y-axis label and format y-axis ticks\n",
    "ax[0].set_ylabel('Sales Price ($)')\n",
    "ax[0].set(ylim=(0, 800000))\n",
    "ax[0].yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Add and format legend\n",
    "price_labels = ['US Median Sales Price', 'Sales Price']\n",
    "\n",
    "# Slice list to remove first handle\n",
    "ax[0].legend(loc='topcenter', frameon=False, labels=price_labels)\n",
    "\n",
    "# Plot median annual ROI by state\n",
    "df.groupby('State')['roi_annual_avg_5y'].median().sort_values(\n",
    "                                                ascending=False).plot(\n",
    "                                                kind='bar',\n",
    "                                                color='paleturquoise',\n",
    "                                                ax=ax[1])\n",
    "\n",
    "# Plot median annual ROI for all states\n",
    "ax[1].axhline(y=np.nanmedian(df.roi_annual_avg_5y), color='black', \n",
    "             linestyle='--')\n",
    "\n",
    "# Set y-axis label\n",
    "ax[1].set_ylabel('ROI (%)')\n",
    "\n",
    "# Add and format legend\n",
    "roi_labels = ['US Median Annual ROI', 'Annual ROI']\n",
    "\n",
    "# Slice list to remove first handle\n",
    "ax[1].legend(labels = roi_labels, \n",
    "             loc='upper right', frameon=False)\n",
    "\n",
    "# Set title\n",
    "plt.suptitle('Median Sales Price and Annual ROI by State', fontsize='13',\n",
    "            y=1.02)\n",
    "\n",
    "# Format plot\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots of 2018 median sales price and average annual ROI over 5-yr period\n",
    "\n",
    "# Create figure and set size\n",
    "fig, ax = plt.subplots(nrows=1, ncols= 2, figsize = (7,5))\n",
    "\n",
    "# Plot boxplots\n",
    "sns.boxplot(data = df['2018-04'], color = 'steelblue', ax=ax[0]).set(\n",
    "    xlabel='Median Sales Price, 2018')\n",
    "sns.boxplot(data = df.roi_annual_avg_5y, color='green', ax=ax[1]).set(\n",
    "    xlabel='ROI, 5-year Annual Average')\n",
    "\n",
    "# Set y-axis range\n",
    "ax[0].set(ylim=(0, 1000000))\n",
    "ax[0].yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "ax[1].set(ylim=(-10,25))\n",
    "\n",
    "# Remove xticklabels and xticks\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].tick_params(bottom=False) \n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].tick_params(bottom=False) \n",
    "\n",
    "# Set title\n",
    "plt.suptitle('Distribution of Sales Price and ROI', y = 1.02)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort df by sales price and ROI criteria for business case\n",
    "# Criteria: sales price between 20th and 50th percentiles and \n",
    "# ROI > 90th percentile\n",
    "df[(df['2018-04'] >= np.percentile(df['2018-04'], 20)) & \n",
    "   (df['2018-04'] < np.percentile(df['2018-04'], 50)) & \n",
    "   (df['roi_annual_avg_5y'] > 10.73)].sort_values(\n",
    "    by='roi_annual_avg_5y', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many zipcodes in Florida emerge among top results according to criteria to focus on below-median sales  \n",
    "price and annual average ROI (over previous 5 years) in the top 90th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df for Florida zipcodes\n",
    "df_fl = df.loc[df['State']=='FL']\n",
    "df_fl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for FL zipcodes annual ROI, 5y period\n",
    "print(df_fl['roi_annual_avg_5y'].describe())\n",
    "print('90%: ', df_fl['roi_annual_avg_5y'].quantile(0.90))\n",
    "print('95%: ', df_fl['roi_annual_avg_5y'].quantile(0.95))\n",
    "print('99%: ', df_fl['roi_annual_avg_5y'].quantile(0.99))\n",
    "print('99.9%: ', df_fl['roi_annual_avg_5y'].quantile(0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for FL zipcodes annual ROI, 3y period\n",
    "print(df_fl['roi_annual_avg_3y'].describe())\n",
    "print('90%: ', df_fl['roi_annual_avg_3y'].quantile(0.90))\n",
    "print('95%: ', df_fl['roi_annual_avg_3y'].quantile(0.95))\n",
    "print('99%: ', df_fl['roi_annual_avg_3y'].quantile(0.99))\n",
    "print('99.9%: ', df_fl['roi_annual_avg_3y'].quantile(0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for FL zipcodes 2018 sales price\n",
    "print(df_fl['2018-04'].describe())\n",
    "print('10%: ', df_fl['2018-04'].quantile(0.10))\n",
    "print('20%: ', df_fl['2018-04'].quantile(0.20))\n",
    "print('90%: ', df_fl['2018-04'].quantile(0.90))\n",
    "print('95%: ', df_fl['2018-04'].quantile(0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot for FL zipcodes annual ROI\n",
    "# Create figure and set size\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,5)) \n",
    "\n",
    "# Plot price boxplot\n",
    "sns.boxplot(df_fl['2018-04'], orient='v', color='steelblue', ax=ax[0]).set(\n",
    "    xlabel='Median Sales Price') \n",
    "\n",
    "# Plot ROI boxplot\n",
    "sns.boxplot(df_fl['roi_annual_avg_3y'], orient='v', color='green', ax=ax[1]\n",
    "           ).set(xlabel='Median ROI')\n",
    "\n",
    "# Set y-axis ranges and labels for subplots\n",
    "ax[0].set(ylim=(0, 1000000))\n",
    "ax[0].yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "ax[0].set_ylabel('Dollars, $')\n",
    "ax[1].set(ylim=(0,25))\n",
    "ax[1].set_ylabel('Percent, %')\n",
    "\n",
    "# Remove xticklabels and xticks\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].tick_params(bottom=False) \n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].tick_params(bottom=False) \n",
    "\n",
    "# Set title\n",
    "plt.suptitle('Florida: Distribution of Sales Price and Annual ROI', y=1.02)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of FL sales price and annual ROI tailored for business case\n",
    "# Create figure and set size\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Scatterplot of median price and roi\n",
    "sns.scatterplot(data=df_fl, x='2018-04', y='roi_annual_avg_5y', color='lightblue')\n",
    "\n",
    "# Plot 50th percentile sales price\n",
    "plt.vlines(226100, 0, 40, color='black', linestyles='dashed',\n",
    "          label='Median sales price ($)')\n",
    "\n",
    "# Plot 50th percentile average annual 5yr ROI\n",
    "plt.hlines(9.85, 0, 1000000, color='green', linestyles='dashed',\n",
    "          label='Median annual ROI (%)')\n",
    "\n",
    "# Set x-axis scale and ticks\n",
    "plt.xlim(0,350000)\n",
    "plt.xticks([50000,100000,150000,200000,250000,300000,350000,400000,450000,\n",
    "            500000,550000,600000,650000,700000,750000,800000,850000,900000,\n",
    "            950000,1000000],\n",
    "           ['50k','100k','150k','200k','250k','300k', '350k', '400k', '450k',\n",
    "           '500k', '550k','600k', '650k', '700k', '750k', '800k', '850k',\n",
    "            '900k', '950k','1,000k'])\n",
    "plt.xlabel('Median Sales Price ($)')\n",
    "\n",
    "# Set y-axis scale and ticks\n",
    "plt.ylim(0, 25)\n",
    "plt.yticks([0, 5, 10, 15, 20, 25])\n",
    "plt.ylabel('ROI, Annual (%)')\n",
    "\n",
    "# Plot box highlighting target group with prices b/w 20th and 50th percentiles\n",
    "# and ROI > 90th percentile and < 99th percentile\n",
    "plt.vlines(160460, 15.995, 19.4, color='red', linestyles='dotted')\n",
    "plt.hlines(15.995, 160460, 226100, color='red', linestyles='dotted')\n",
    "#plt.hlines(17.87, 160460, 226100, color='red', linestyles='dotted')\n",
    "plt.hlines(19.4, 160460, 226100, color='red', linestyles='dotted')\n",
    "\n",
    "# Add and format legend\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Add title\n",
    "plt.title('Florida: Median Sales Price vs Annual ROI')\n",
    "\n",
    "# Remove borders\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset df according to selection criteria:\n",
    "# State of Florida \n",
    "# Sales price between 20th and 50th percentiles and \n",
    "# ROI > 90 percentile and < 99th percentiles (to remove 'hot market' outliers)\n",
    "# Select top 10 zipcodes based on annual avg ROI over 5y period.\n",
    "zillow_select = df_fl[(df_fl['2018-04'] >= \n",
    "                       np.percentile(df_fl['2018-04'], 20)) & \n",
    "                      (df_fl['2018-04'] < np.percentile(df['2018-04'], 50)) & \n",
    "                      (df_fl['roi_annual_avg_3y'] > 17.3) & \n",
    "                      (df['roi_annual_avg_3y'] < 23.2)].sort_values(\n",
    "    by='roi_annual_avg_3y', ascending=False).head(10)\n",
    "zillow_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset df according to selection criteria:  **SORT BY 3Y ANNUAL ROI TO GET ZIPCODES WITH UPWARD MOMENTUM?**\n",
    "# State of Florida \n",
    "# Sales price between 20th and 50th percentiles and \n",
    "# ROI > 90 percentile and < 99th percentiles (to remove 'hot market' outliers)\n",
    "# Select top 10 zipcodes based on annual avg ROI over 5y period.\n",
    "zillow_select = df_fl[(df_fl['2018-04'] >= \n",
    "                       np.percentile(df_fl['2018-04'], 20)) & \n",
    "                      (df_fl['2018-04'] < np.percentile(df['2018-04'], 50)) & \n",
    "                      (df_fl['roi_annual_avg_5y'] > 15.955) & \n",
    "                      (df['roi_annual_avg_5y'] < 19.4)].sort_values(\n",
    "    by='roi_annual_avg_5y', ascending=False).head(10)\n",
    "zillow_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime dtype\n",
    "# Starter function\n",
    "def get_datetimes(df):\n",
    "    return pd.to_datetime(df.columns.values[:], format='%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select date columns only and pass them through function\n",
    "zillow_select_date_df = zillow_select.iloc[:,8:-8]\n",
    "zillow_select_date_df.columns = list(get_datetimes(zillow_select_date_df))\n",
    "# Check dtype, which should be datetime\n",
    "zillow_select_date_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat dfs (7 informational columns, 9 roi columns, date columns)\n",
    "zillow_select_df = pd.concat([zillow_select.iloc[:, :7], zillow_select.iloc[:, -8:], zillow_select_date_df], axis=1)\n",
    "zillow_select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all columns\n",
    "list(zillow_select_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df for each zipcode selected for this phase of analysis\n",
    "pierson = zillow_select_df[zillow_select_df['Zipcode']=='32180']\n",
    "egyptlakeleto = zillow_select_df[zillow_select_df['Zipcode']=='33614']\n",
    "stpete = zillow_select_df[zillow_select_df['Zipcode']=='33713']\n",
    "orlando = zillow_select_df[zillow_select_df['Zipcode']=='32839']\n",
    "bradenton = zillow_select_df[zillow_select_df['Zipcode']=='34205']\n",
    "pompanobeach = zillow_select_df[zillow_select_df['Zipcode']=='33069']\n",
    "jacksonville = zillow_select_df[zillow_select_df['Zipcode']=='32204']\n",
    "westpalmbeach = zillow_select_df[zillow_select_df['Zipcode']=='33407']\n",
    "lockhart = zillow_select_df[zillow_select_df['Zipcode']=='32810']\n",
    "portcharlotte = zillow_select_df[zillow_select_df['Zipcode']=='33948']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_list = [pierson, egyptlakeleto, stpete, orlando, bradenton,\n",
    "           pompanobeach, jacksonville, westpalmbeach, lockhart, portcharlotte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "stpete.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape from wide to long format\n",
    "def melt_data(df):\n",
    "    \"\"\"\n",
    "    Takes the zillow_data dataset in wide form or a subset of the zillow_dataset.  \n",
    "    Returns a long-form datetime dataframe \n",
    "    with the datetime column names as the index and the values as the 'values' column.\n",
    "    \n",
    "    If more than one row is passes in the wide-form dataset, the values column\n",
    "    will be the mean of the values from the datetime columns in all of the rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    melted = pd.melt(df, id_vars=['RegionID','Zipcode','City','State','Metro',\n",
    "                                  'County','SizeRank','roi_cum','roi_13-14',\n",
    "                                  'roi_14-15','roi_15-16','roi_16-17',\n",
    "                                  'roi_17-18','roi_annual_avg_5y',\n",
    "                                  'roi_annual_avg_3y'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted.groupby('time').aggregate({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to melt dfs\n",
    "pierson_ts_full = melt_data(pierson)\n",
    "egyptlakeleto_ts_full = melt_data(egyptlakeleto)\n",
    "stpete_ts_full = melt_data(stpete)\n",
    "orlando_ts_full = melt_data(orlando)\n",
    "bradenton_ts_full = melt_data(bradenton)\n",
    "pompanobeach_ts_full = melt_data(pompanobeach)\n",
    "jacksonville_ts_full = melt_data(jacksonville)\n",
    "westpalmbeach_ts_full = melt_data(westpalmbeach)\n",
    "lockhart_ts_full = melt_data(lockhart)\n",
    "portcharlotte_ts_full = melt_data(portcharlotte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with all melted city dfs and plot all series in one graph\n",
    "# List of time series dfs\n",
    "ts_list = [pierson_ts_full, egyptlakeleto_ts_full, stpete_ts_full, orlando_ts_full, \n",
    "           bradenton_ts_full, pompanobeach_ts_full, jacksonville_ts_full, \n",
    "           westpalmbeach_ts_full, lockhart_ts_full, portcharlotte_ts_full]\n",
    "\n",
    "# Concat dfs into one df\n",
    "ts_full_df = pd.concat(ts_list, axis=1)\n",
    "\n",
    "# Rename columns\n",
    "names = ['Pierson', 'Egypt Lake Leto', 'St. Petersburg', 'Orlando', \n",
    "             'Bradenton', 'Pompano Beach', 'Jacksonville', 'West Palm Beach',\n",
    "            'Lockhart', 'Port Charlotte']\n",
    "ts_full_df.columns = names\n",
    "\n",
    "# Plot series\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ts_df.plot(ax=ax)\n",
    "# Set x and y axis labels\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Median House Price ($)')\n",
    "# Format y-ticks\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Set title\n",
    "plt.title('Florida: Median House Prices Over Time')\n",
    "# Format legend\n",
    "ax.legend(loc='upper left', frameon=False)\n",
    "# Remove borders\n",
    "sns.despine()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Times series are not stationary. There appear to be cyclical and upward trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pierson_ts.plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter time series to focus on post housing crisis period 2008-10\n",
    "# pierson_ts = pierson_ts_full['2011':'2018']\n",
    "# pierson_ts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to apply train_test_split to all selected dfs\n",
    "def tt_split(df_list, names):\n",
    "    tts_list = []\n",
    "    \n",
    "    for i, df in enumerate(df_list):\n",
    "        train_data = df[:-12] #all data, except last 12 months\n",
    "        test_data = df[-12:] #last 12 months of data\n",
    "        tts_list.extend([train_data, test_data])\n",
    "        print(names[i],':', df.shape, ' // Train: ', train_data.shape,\n",
    "             'Test: ', test_data.shape)\n",
    "    return tts_list    #return list of train, test dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt_split(ts_list, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pierson_train, pierson_test, egyptlakeleto_train, egyptlakeleto_test, \\\n",
    "    stpete_train, stpete_test, orlando_train, orlando_test, \\\n",
    "    bradenton_train, bradenton_test, pompanobeach_train, pompanobeach_test,\\\n",
    "    jacksonville_train, jacksonville_test, westpalmbeach_train, westpalmbeach_test,\\\n",
    "    lockhart_train, lockhart_test, portcharlotte_train, portcharlotte_test \\\n",
    "    = tt_split(ts_list, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train list and test list\n",
    "train_dfs = [pierson_train, egyptlakeleto_train, stpete_train, orlando_train,\n",
    "             bradenton_train, pompanobeach_train, jacksonville_train,\n",
    "             westpalmbeach_train, lockhart_train, portcharlotte_train]\n",
    "test_dfs = [pierson_test, egyptlakeleto_test, stpete_test, orlando_test,\n",
    "             bradenton_test, pompanobeach_test, jacksonville_test,\n",
    "             westpalmbeach_test, lockhart_test, portcharlotte_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot autocorrelation function (ACF) and \n",
    "# partial autocorrelation function (PACF)\n",
    "\n",
    "def plot_acf_pacf(ts):\n",
    "    # Create figure, set format and size\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "    plot_acf(ts, ax[0]) #plot ACF\n",
    "    ax[0].xaxis.set_major_locator(plt.MultipleLocator(2)) #set xtick frequency\n",
    "    ax[0].set_xlabel('Lag') #set xlabel\n",
    "    ax[0].set_ylabel('Autocorrelation') #set ylabel\n",
    "\n",
    "    plot_pacf(pierson_ts, ax[1]) #plot PACF\n",
    "    ax[1].set_xlim(-1,21) #set x-axis range\n",
    "    ax[1].xaxis.set_major_locator(plt.MultipleLocator(1)) #set xtick frequency\n",
    "    ax[1].set_xlabel('Lag') #set xlabel\n",
    "    ax[1].set_ylabel('Partial Autocorrelation') #set ylabel\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to check for stationarity\n",
    "def stationarity_check(ts):\n",
    "    \"\"\"Calculate 12m rolling statistics and plot against original time series, \n",
    "    and perform and return Dickey Fuller test\"\"\"\n",
    "    \n",
    "    print('Results of Dickey-Fuller Test: \\n')\n",
    "        \n",
    "    # Calculate rolling mean and standard deviation\n",
    "    rolling_mean = ts.rolling(window=12, center=False).mean()\n",
    "    rolling_std = ts.rolling(window=12, center=False).std()\n",
    "        \n",
    "    # Perform the Dickey Fuller Test\n",
    "    df_test = adfuller(ts['value'])\n",
    "    \n",
    "    # Plot rolling statistics\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    plt.plot(ts, color='black', label='Original')\n",
    "    plt.plot(rolling_mean.dropna(), color='blue', label='Rolling Mean, 12-month', \n",
    "             linestyle='dotted')\n",
    "    plt.plot(rolling_std.dropna(), color='green', \n",
    "             label = 'Rolling Std, 12-month',\n",
    "             linestyle='dotted')\n",
    "    \n",
    "    # Format y-ticks\n",
    "    plt.gca().yaxis.set_major_formatter(StrMethodFormatter('${x:,.0f}'))\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='best')\n",
    "    # Add title\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Print Dickey-Fuller test results\n",
    "    print('Results of Dickey-Fuller Test: \\n')\n",
    "    df_output = pd.Series(df_test[0:4], index=['Test Statistic', 'p-value', \n",
    "                                             'Number of Lags Used', \n",
    "                                             'Number of Observations Used'])\n",
    "    for key,value in df_test[4].items():\n",
    "        df_output['Critical Value (%s)'%key] = value\n",
    "    print(df_output)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fit baseline SARIMAX model and print results\n",
    "# p = 1, d = 1 , m = 1, s=12\n",
    "\n",
    "def fit_baseline_sarimax_model(metrics_df, name, train, order=(1,1,1),\n",
    "                               seasonal_order=(0,0,0,12)):\n",
    "    \n",
    "    # Instantiate model\n",
    "    arima_model = sm.tsa.statespace.SARIMAX(train,\n",
    "                                           order=order,\n",
    "                                           seasonal_order=seasonal_order)\n",
    "    # Fit model\n",
    "    output = arima_model.fit()\n",
    "    \n",
    "    # obtain comp metrics\n",
    "    comp_metrics = [name, order, seasonal_order]\n",
    "    # append AIC score\n",
    "    comp_metrics.append(round(output.aic, 2))\n",
    "    # add selected output metrics to df\n",
    "    metrics_columns = ['Name', 'Baseline_Order', 'Baseline_Seasonal_Order','Baseline_AIC_Score']\n",
    "    metrics_df = pd.DataFrame(comp_metrics).T\n",
    "    metrics_df.columns = metrics_columns\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "    # Print results\n",
    "    #display(output.summary())\n",
    "    #output.plot_diagnostics(figsize=(11,8))\n",
    "    \n",
    "    #return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to find best parameters for SARIMAX model using auto_arima\n",
    "\"\"\"parameters: m=12 months, trend='ct' (constant and linear), time series is \n",
    "seasonal and is not stationary\"\"\"\n",
    "\n",
    "def find_params(name, ts):  \n",
    "    params_output = pm.auto_arima(ts, start_p=1, start_q=1, max_p=4, max_q=4,\n",
    "                                m=12, seasonal=True, stationary=False,\n",
    "                                stepwise=True, trend='ct',\n",
    "                                information_criterion='aic',  \n",
    "                                suppress_warnings=True, trace=False, \n",
    "                                error_action='ignore')\n",
    "   \n",
    "    return name, params_output.order, params_output.seasonal_order, round(\n",
    "        params_output.aic(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fit SARIMAX model and print results\n",
    "# p = 1, d = 1, m = 1 per ACF, PCF plots and ADF stationarity check\n",
    "\n",
    "def fit_sarimax_model(ts, order=(1,1,1), seasonal_order=(0,0,0,12)):\n",
    "    # Instantiate model\n",
    "    arima_model = sm.tsa.statespace.SARIMAX(ts,\n",
    "                                           order=order,\n",
    "                                           seasonal_order=seasonal_order,\n",
    "                                           enforce_stationarity=True,\n",
    "                                           enforce_invertibility=False)\n",
    "    # Fit model\n",
    "    output = arima_model.fit()\n",
    "    \n",
    "    # Print results\n",
    "    display(output.summary())\n",
    "    output.plot_diagnostics(figsize=(11,8))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(train, test, model_output, steps=24):\n",
    "\n",
    "    # Calculate predictions from 2017-05 and calculate 95% confidence interval\n",
    "    prediction = model_output.get_prediction(start=pd.to_datetime('2017-05'),\n",
    "                                             end=pd.to_datetime('2018-04'),\n",
    "                                            dynamic=True)\n",
    "    pred_ci = prediction.conf_int(alpha=0.05)\n",
    "    \n",
    "    # Calculate forecast and 95% confidence interval for steps in future\n",
    "    future = model_output.get_forecast(steps=steps, dynamic=True)\n",
    "    future_ci = future.conf_int(steps=steps)\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "    # Subplot 0: observed v predicted values\n",
    "    # Plot observed values\n",
    "    all_data = pd.concat([train, test], axis=0)\n",
    "    all_data['2011':].plot(label='Actual', color='blue', ax=ax[0])\n",
    "\n",
    "    # Plot predicted values\n",
    "    prediction.predicted_mean.plot(color='green', \n",
    "                               label='Predicted', linestyle='dotted', \n",
    "                               ax=ax[0])\n",
    "\n",
    "    # Plot the confidence interval\n",
    "    ax[0].fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='gray', alpha=.15)\n",
    "\n",
    "    # Format y-axis range\n",
    "    ax[0].set_ylim(0,300000)\n",
    "    # Set axes labels\n",
    "    ax[0].set_ylabel('Mean Sales Price ($)')\n",
    "    ax[0].set_xlabel('')\n",
    "    # Format ticks\n",
    "    ax[0].yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "    # Subplot 1: forecasted values\n",
    "    # Plot observed values\n",
    "    all_data['2011':].plot(label='Actual', color='blue', ax=ax[1])\n",
    "    # Plot forecasted values\n",
    "    future.predicted_mean.plot(label='Forecast', color='darkorange', \n",
    "                           linestyle='dotted', ax=ax[1])\n",
    "\n",
    "    # Prediction for forecast period and confidence interval\n",
    "    forecast = future.predicted_mean[-1]\n",
    "    maximum = future_ci.iloc[-1,1]\n",
    "    minimum = future_ci.iloc[-1,0]\n",
    "    predictions = {}\n",
    "    predictions['forecast'] = forecast\n",
    "    predictions['maximum'] = maximum\n",
    "    predictions['minimum'] = minimum\n",
    "\n",
    "    ax[1].fill_between(future_ci.index,\n",
    "                    future_ci.iloc[:, 0],\n",
    "                    future_ci.iloc[:, 1], color='gray', alpha=.15)\n",
    "    \n",
    "    # Format y-axis range\n",
    "    ax[1].set_ylim(0,300000)\n",
    "    # Set axes labels\n",
    "    ax[1].set_ylabel('Mean Sales Price ($)')\n",
    "    ax[1].set_xlabel('')\n",
    "    # Format ticks\n",
    "    ax[1].yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "    # Set titles and legend\n",
    "    plt.suptitle('Actual vs Predicted Mean Sales Price', y=1.02)\n",
    "    ax[0].legend(['Actual', 'Predicted'], loc='upper left')\n",
    "    ax[1].legend(['Actual', 'Predicted'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "    # Calculate RMSE for last year of actual data vs prediction\n",
    "    # Use test set for last year of data\n",
    "    rmse = (np.sqrt((test.value - prediction.predicted_mean) ** 2)).mean()\n",
    "    print('Root Mean Squared Error of predictions is ${}'.format(round(rmse, 2)))\n",
    "    \n",
    "    # Print forecasts\n",
    "    #print('Forecasted values: ')\n",
    "    #print(future.predicted_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PCF for all time series\n",
    "for i, train_df in enumerate(train_dfs): \n",
    "    plot_acf_pacf(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time series have comparable ACF and PACF plots.\n",
    "- In ACF plot, previous sales price influences current sales price, but the significance decreases slowly.  \n",
    "The first 18 lags are significant at 5% significance level (95% confidence interval).  \n",
    "- The PACF plot indicates that partial autocorrelations for lags 0, 1, and 2 are statistically significant.  \n",
    "Thus, the PACF suggests fitting a first- or second-order autoregressive (AR) model. The lag of 1  \n",
    "has significant spike and then drops off.\n",
    "- Autoregressive model is appropriate. According to PACF, the 'p' component could have a value of 1,  \n",
    "which is value after which there is a sharp decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for stationarity\n",
    "stationarity_check(pierson_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- p-value > 0.05 critical value. Time series is not stationary; there are trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference the time series and recheck stationarity of differenced series\n",
    "pierson_train_diff = pierson_train.diff(7).dropna()\n",
    "stationarity_check(pierson_train_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With differencing factor of 7, p-value is < 0.05 critical value. Time series is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline SARIMAX model for all dfs, with ar(1), d(1), and ma(1)\n",
    "comp_results = []\n",
    "for i, train_df in enumerate(train_dfs): \n",
    "    baseline_comp_df = fit_baseline_sarimax_model(metrics_df, names[i], train_df)   \n",
    "    comp_results.append(baseline_comp_df)\n",
    "    baseline_comp_df = pd.concat(comp_results)\n",
    "print('Baseline Model')\n",
    "baseline_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameters: optimal p,d,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function to find best p,d,q,s parameters\n",
    "comp_results_2 = []\n",
    "    \n",
    "for i, train_df in enumerate(train_dfs):\n",
    "    best_comp_df = find_params(names[i], train_df)\n",
    "    comp_results_2.append(best_comp_df)\n",
    "    metrics_columns = ['Name','Best_Order', 'Best_Seasonal_Order', 'Best_AIC_Score']\n",
    "    best_comp_df = pd.DataFrame(comp_results_2)\n",
    "    best_comp_df.columns = metrics_columns\n",
    "    \n",
    "best_comp_df \n",
    "\n",
    "# metrics_columns = ['Name', 'Order', 'Seasonal_Order','AIC Score']\n",
    "#     metrics_df = pd.DataFrame(comp_metrics).T\n",
    "#     metrics_df.columns = metrics_columns\n",
    "\n",
    "# comp_results = []\n",
    "# for i, train_df in enumerate(train_dfs): \n",
    "#     baseline_comp_df = fit_baseline_sarimax_model(metrics_df, names[i], train_df)   \n",
    "#     comp_results.append(baseline_comp_df)\n",
    "#     baseline_comp_df = pd.concat(comp_results)\n",
    "# print('Baseline Model')\n",
    "# baseline_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df with comparative results\n",
    "comparative_df = pd.merge(baseline_comp_df, best_comp_df, on='Name')\n",
    "comparative_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_df['AIC_Comp'] = comparative_df['Best_AIC_Score'] - comparative_df['Baseline_AIC_Score']\n",
    "comparative_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first zipcode through function to fit SARIMAX model\n",
    "# Parameters obtained from best estimates given ACF and PACF plots, ADF stationarity check\n",
    "pierson_output = fit_sarimax_model(pierson_train, order=(1,1,1), \n",
    "                                   seasonal_order=(0,0,0,12))\n",
    "\n",
    "# predictions from model with inputs based on ACF, PACF plots\n",
    "pierson_predictions = get_predictions(pierson_train, pierson_test, \n",
    "                                      pierson_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run time series through function to fit SARIMAX model using best params \n",
    "pierson_output_2 = fit_sarimax_model(pierson_train, order=(4,0,0), \n",
    "                                   seasonal_order=(0,0,2,12))\n",
    "\n",
    "# predictions from model with inputs based on best_params auto_arima function\n",
    "pierson_predictions_2 = get_predictions(pierson_train, pierson_test, \n",
    "                                        pierson_output_2, steps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model with inputs from best params function has better distribution of residuals and lower RMSE than  \n",
    "model with inputs inferred from ACF/PACF, despite them both having similar AIC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(egyptlakeleto_train.info())\n",
    "print(pierson_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Egypt Lake Leto\n",
    "# Run time series through function to fit SARIMAX model using best params \n",
    "egyptlakeleto_output = fit_sarimax_model(egyptlakeleto_train, order=(4,0,2), \n",
    "                                   seasonal_order=(1,0,2,12))\n",
    "\n",
    "# predictions from model with inputs based on best_params auto_arima function\n",
    "egyptlakeleto_predictions = get_predictions(egyptlakeleto_train, egyptlakeleto_test, \n",
    "                                        egyptlakeleto_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## St. Petersberg\n",
    "# Fit model using best_params\n",
    "stpete_output = fit_sarimax_model(stpete_train, order=(0,0,0), \n",
    "                                   seasonal_order=(0,0,2,12))\n",
    "# Get predictions\n",
    "get_predictions(stpete_train, stpete_test, stpete_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Orlando\n",
    "# Fit model using best_params\n",
    "orlando_output = fit_sarimax_model(orlando_train, order=(4,0,2), \n",
    "                                   seasonal_order=(2,0,2,12))\n",
    "# Get predictions\n",
    "get_predictions(orlando_train, orlando_test, orlando_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bradenton\n",
    "# Fit model using best_params\n",
    "bradenton_output = fit_sarimax_model(bradenton_train, order=(2,0,3), \n",
    "                                   seasonal_order=(0,0,2,12))\n",
    "# Get predictions\n",
    "get_predictions(bradenton_train, bradenton_test, bradenton_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egyptlakeleto_output = fit_sarimax_model(egyptlakeleto_train, order=(4,0,2), \n",
    "                                   seasonal_order=(2,0,2,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pompano Beach\n",
    "# Fit model using best_params\n",
    "pompanobeach_output = fit_sarimax_model(pompanobeach_train, order=(4,0,2), \n",
    "                                   seasonal_order=(0,0,2,12))\n",
    "# Get predictions\n",
    "get_predictions(pompanobeach_train, pompanobeach_test, pompanobeach_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Jacksonville\n",
    "# Fit model using best_params\n",
    "jacksonville_output = fit_sarimax_model(jacksonville_train, order=(2,0,4), \n",
    "                                   seasonal_order=(0,0,2,12))\n",
    "# Get predictions\n",
    "get_predictions(jacksonville_train, jacksonville_test, jacksonville_output, steps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- price vs ROI as X variable?\n",
    "- evaluation metric: RMSE\n",
    "- ACF/PACF vs auto_arima or other function to determine best params?\n",
    "- what if ACF, PACF params differ from auto_arima best params?\n",
    "- differencing\n",
    "- loop for baseline model and passing results to df\n",
    "\n",
    "\n",
    "- RMSE for predictions vs observed and/or forecasts vs. what value?\n",
    "\n",
    "- when/how to do train-test-split?\n",
    "\n",
    "- function to loop through all ts? store all values in df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
